{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17406c93-1f01-41a8-94fe-fb34c331c723",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dccd4413-7a9c-4d59-aeef-363a43a5c8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://www.osha.gov/ords/imis/accidentsearch.search\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05212624-48ee-4cf6-99b9-4595a1d1cff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "    'accept-language': 'en-US,en;q=0.9',\n",
    "    'cache-control': 'max-age=0',\n",
    "    'priority': 'u=0, i',\n",
    "    'referer': 'https://www.osha.gov/ords/imis/accidentsearch.search?sic=&sicgroup=&naics=&acc_description=heat&acc_abstract=&acc_keyword=&inspnr=&fatal=&officetype=All&office=All&startmonth=10&startday=31&startyear=2024&endmonth=01&endday=01&endyear=2000&keyword_list=&p_start=&p_finish=80&p_sort=&p_desc=DESC&p_direction=Next&p_show=20',\n",
    "    'sec-ch-ua': '\"Google Chrome\";v=\"131\", \"Chromium\";v=\"131\", \"Not_A Brand\";v=\"24\"',\n",
    "    'sec-ch-ua-mobile': '?0',\n",
    "    'sec-ch-ua-platform': '\"macOS\"',\n",
    "    'sec-fetch-dest': 'document',\n",
    "    'sec-fetch-mode': 'navigate',\n",
    "    'sec-fetch-site': 'same-origin',\n",
    "    'sec-fetch-user': '?1',\n",
    "    'upgrade-insecure-requests': '1',\n",
    "    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',\n",
    "}\n",
    "\n",
    "cookies = {\n",
    "    '_ga': 'GA1.1.685929613.1730142385',\n",
    "    '_ga_CSLL4ZEK4L': 'GS1.1.1732388998.32.1.1732392494.0.0.0',\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'sic': '',\n",
    "    'sicgroup': '',\n",
    "    'naics': '',\n",
    "    'acc_description': 'heat',\n",
    "    'acc_abstract': '',\n",
    "    'acc_keyword': '',\n",
    "    'inspnr': '',\n",
    "    'fatal': '',\n",
    "    'officetype': 'All',\n",
    "    'office': 'All',\n",
    "    'startmonth': '10',\n",
    "    'startday': '31',\n",
    "    'startyear': '2024',\n",
    "    'endmonth': '01',\n",
    "    'endday': '01',\n",
    "    'endyear': '2000',\n",
    "    'keyword_list': '',\n",
    "    'p_sort': '',\n",
    "    'p_desc': 'DESC',\n",
    "    'p_direction': 'Next',\n",
    "    'p_show': '20',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1b58c8e-a6e9-4f35-9216-c338fa8be325",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_numbers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09804cdb-1e84-4f67-be56-8e8223b683d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page with p_finish=0\n",
      "Collected 20 summary numbers from all pages.\n",
      "['167601.015', '171242.015', '167481.015', '167213.015', '166832.015', '171335.015', '165828.015', '162263.015', '160090.015', '160100.015', '159481.015', '159448.015', '159496.015', '159249.015', '159560.015', '159454.015', '159272.015', '159294.015', '159031.015', '159516.015']\n"
     ]
    }
   ],
   "source": [
    "for page in range(0, 61 * 20, 20):\n",
    "    print(f\"Scraping page with p_finish={page}\")\n",
    "    params['p_finish'] = str(page)\n",
    "    \n",
    "    response = requests.get(base_url, params=params, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    page_numbers = [\n",
    "        a_tag.text.strip()\n",
    "        for a_tag in soup.find_all('a', href=True)\n",
    "        if \"accidentsearch.accident_detail\" in a_tag['href']\n",
    "    ]\n",
    "\n",
    "    if not page_numbers:\n",
    "        print(\"No more pages. Stopped.\")\n",
    "        break\n",
    "\n",
    "    summary_numbers.extend(page_numbers)\n",
    "\n",
    "    time.sleep(5)\n",
    "\n",
    "print(f\"Collected {len(summary_numbers)} summary numbers from all pages.\")\n",
    "print(summary_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17c64b7d-b5ed-44af-94af-c6a26eadda82",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://www.osha.gov/ords/imis/accidentsearch.accident_detail\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89f77cfa-c8da-4d2b-9026-0113dd58b8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "inspection_numbers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff31438d-3ee7-44fd-a1f9-7b8c208570f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping inspection number for summary ID: 167601.015\n",
      "Found inspection number: 1756686.015\n",
      "Scraping inspection number for summary ID: 171242.015\n",
      "Found inspection number: 1754345.015\n",
      "Scraping inspection number for summary ID: 167481.015\n",
      "Found inspection number: 1755843.015\n",
      "Scraping inspection number for summary ID: 167213.015\n",
      "Found inspection number: 1753424.015\n",
      "Scraping inspection number for summary ID: 166832.015\n",
      "Found inspection number: 1750189.015\n",
      "Scraping inspection number for summary ID: 171335.015\n",
      "Found inspection number: 1750111.015\n",
      "Scraping inspection number for summary ID: 165828.015\n",
      "Found inspection number: 1742027.015\n",
      "Scraping inspection number for summary ID: 162263.015\n",
      "Found inspection number: 1715319.015\n",
      "Scraping inspection number for summary ID: 160090.015\n",
      "Found inspection number: 1698609.015\n",
      "Scraping inspection number for summary ID: 160100.015\n",
      "Found inspection number: 1698692.015\n",
      "Scraping inspection number for summary ID: 159481.015\n",
      "Found inspection number: 1694606.015\n",
      "Scraping inspection number for summary ID: 159448.015\n",
      "Found inspection number: 1693972.015\n",
      "Scraping inspection number for summary ID: 159496.015\n",
      "Found inspection number: 1694715.015\n",
      "Scraping inspection number for summary ID: 159249.015\n",
      "Found inspection number: 1692722.015\n",
      "Scraping inspection number for summary ID: 159560.015\n",
      "Found inspection number: 1695215.015\n",
      "Scraping inspection number for summary ID: 159454.015\n",
      "Found inspection number: 1694385.015\n",
      "Scraping inspection number for summary ID: 159272.015\n",
      "Found inspection number: 1692708.015\n",
      "Scraping inspection number for summary ID: 159294.015\n",
      "Found inspection number: 1693210.015\n",
      "Scraping inspection number for summary ID: 159031.015\n",
      "Found inspection number: 1690736.015\n",
      "Scraping inspection number for summary ID: 159516.015\n",
      "Found inspection number: 1694845.015\n",
      "Collected 20 inspection numbers:\n",
      "['1756686.015', '1754345.015', '1755843.015', '1753424.015', '1750189.015', '1750111.015', '1742027.015', '1715319.015', '1698609.015', '1698692.015', '1694606.015', '1693972.015', '1694715.015', '1692722.015', '1695215.015', '1694385.015', '1692708.015', '1693210.015', '1690736.015', '1694845.015']\n"
     ]
    }
   ],
   "source": [
    "for summary_number in summary_numbers:\n",
    "    print(f\"Scraping inspection number for summary ID: {summary_number}\")\n",
    "    params = {'id': summary_number}\n",
    "    \n",
    "    response = requests.get(base_url, params=params, headers=headers, cookies=cookies)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error fetching page for {summary_number}: HTTP {response.status_code}\")\n",
    "        print(response.text)\n",
    "        continue\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    found = False\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        if \"establishment.inspection_detail\" in link['href']:\n",
    "            inspection_number = link.text.strip()\n",
    "            inspection_numbers.append(inspection_number)\n",
    "            print(f\"Found inspection number: {inspection_number}\")\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        print(f\"No inspection number found for summary ID: {summary_number}\")\n",
    "    \n",
    "    time.sleep(5)\n",
    "\n",
    "print(f\"Collected {len(inspection_numbers)} inspection numbers:\")\n",
    "print(inspection_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bd8ed00-8c7f-4d2d-85cf-68e227653e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://www.osha.gov/ords/imis/establishment.inspection_detail?id=\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76158de3-37aa-450b-983a-b56e7c2c121b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(tag):\n",
    "    return tag.text.strip() if tag else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5abb21f-0295-4dba-a654-5a2ca5097726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_with_retries(url, headers, retries=3, backoff_factor=2):\n",
    "    for attempt in range(retries):\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            return response\n",
    "        elif response.status_code == 403:\n",
    "            print(f\"403 Forbidden on attempt {attempt + 1}. Retrying...\")\n",
    "        else:\n",
    "            print(f\"HTTP {response.status_code} on attempt {attempt + 1}. Retrying...\")\n",
    "        time.sleep(backoff_factor ** attempt)\n",
    "    print(f\"Failed to fetch {url} after {retries} attempts.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7464130-cf13-4d87-9aa3-31d1bbc5a746",
   "metadata": {},
   "outputs": [],
   "source": [
    "OSHA_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85573eef-641a-43b8-98f8-3ab661d87440",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = len(inspection_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb4b3810-0b19-4d1c-9f91-6c896cf66cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping details for Inspection ID: 1756686.015 (1/20)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c9/5dg8dy915gl599_ymwjvm4p80000gn/T/ipykernel_36540/1928092163.py:49: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  abstract_div = soup.find('div', class_='span4', text=lambda t: t and \"Employee\" in t)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details collected for Inspection ID: 1756686.015 (1/20)\n",
      "Scraping details for Inspection ID: 1754345.015 (2/20)...\n",
      "Details collected for Inspection ID: 1754345.015 (2/20)\n",
      "Scraping details for Inspection ID: 1755843.015 (3/20)...\n",
      "Details collected for Inspection ID: 1755843.015 (3/20)\n",
      "Scraping details for Inspection ID: 1753424.015 (4/20)...\n",
      "Details collected for Inspection ID: 1753424.015 (4/20)\n",
      "Scraping details for Inspection ID: 1750189.015 (5/20)...\n",
      "Details collected for Inspection ID: 1750189.015 (5/20)\n",
      "Scraping details for Inspection ID: 1750111.015 (6/20)...\n",
      "Details collected for Inspection ID: 1750111.015 (6/20)\n",
      "Scraping details for Inspection ID: 1742027.015 (7/20)...\n",
      "Details collected for Inspection ID: 1742027.015 (7/20)\n",
      "Scraping details for Inspection ID: 1715319.015 (8/20)...\n",
      "Details collected for Inspection ID: 1715319.015 (8/20)\n",
      "Scraping details for Inspection ID: 1698609.015 (9/20)...\n",
      "Details collected for Inspection ID: 1698609.015 (9/20)\n",
      "Scraping details for Inspection ID: 1698692.015 (10/20)...\n",
      "Details collected for Inspection ID: 1698692.015 (10/20)\n",
      "Scraping details for Inspection ID: 1694606.015 (11/20)...\n",
      "Details collected for Inspection ID: 1694606.015 (11/20)\n",
      "Scraping details for Inspection ID: 1693972.015 (12/20)...\n",
      "Details collected for Inspection ID: 1693972.015 (12/20)\n",
      "Scraping details for Inspection ID: 1694715.015 (13/20)...\n",
      "Details collected for Inspection ID: 1694715.015 (13/20)\n",
      "Scraping details for Inspection ID: 1692722.015 (14/20)...\n",
      "Details collected for Inspection ID: 1692722.015 (14/20)\n",
      "Scraping details for Inspection ID: 1695215.015 (15/20)...\n",
      "Details collected for Inspection ID: 1695215.015 (15/20)\n",
      "Scraping details for Inspection ID: 1694385.015 (16/20)...\n",
      "Details collected for Inspection ID: 1694385.015 (16/20)\n",
      "Scraping details for Inspection ID: 1692708.015 (17/20)...\n",
      "Details collected for Inspection ID: 1692708.015 (17/20)\n",
      "Scraping details for Inspection ID: 1693210.015 (18/20)...\n",
      "Details collected for Inspection ID: 1693210.015 (18/20)\n",
      "Scraping details for Inspection ID: 1690736.015 (19/20)...\n",
      "Details collected for Inspection ID: 1690736.015 (19/20)\n",
      "Scraping details for Inspection ID: 1694845.015 (20/20)...\n",
      "Details collected for Inspection ID: 1694845.015 (20/20)\n",
      "Data saved to inspection_details.csv\n"
     ]
    }
   ],
   "source": [
    "for index, inspection_id in enumerate(inspection_numbers, start=1):\n",
    "    print(f\"Scraping details for Inspection ID: {inspection_id} ({index}/{total})...\")\n",
    "\n",
    "    response = fetch_with_retries(base_url + inspection_id, headers)\n",
    "    if not response:\n",
    "        OSHA_data.append({'Inspection Number': inspection_id})\n",
    "        continue\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    try:\n",
    "        case_status = extract_text(soup.find('div', class_='well well-small').find('strong'))\n",
    "        if case_status:\n",
    "            case_status = case_status.replace(\"Case Status:\", \"\").strip()\n",
    "\n",
    "        establishment_name_tag = soup.find('h4', id=inspection_id)\n",
    "        establishment_name = extract_text(establishment_name_tag).split(\"-\", 1)[-1].strip() if establishment_name_tag else None\n",
    "\n",
    "        report_id = None\n",
    "        try:\n",
    "            report_id = soup.find('strong', string=\"Report ID\").next_sibling.strip().replace(\": \", \"\")\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "        site_address = None\n",
    "        try:\n",
    "            span4_divs = soup.find_all('div', class_='span4')\n",
    "            for div in span4_divs:\n",
    "                paragraphs = div.find_all('p')\n",
    "                for paragraph in paragraphs:\n",
    "                    strong_tag = paragraph.find('strong')\n",
    "                    if strong_tag and \"Site Address\" in strong_tag.text:\n",
    "                        address_lines = [line.strip() for line in paragraph.stripped_strings]\n",
    "                        site_address = \"\\n\".join(line.lstrip(\":\").strip() for line in address_lines[3:])\n",
    "                        break\n",
    "                if site_address:\n",
    "                    break\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting Site Address: {e}\")\n",
    "\n",
    "        union_status = None\n",
    "        try:\n",
    "            union_status = soup.find('strong', string=\"Union Status\").next_sibling.strip().replace(\": \", \"\")\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "        incident_title = None\n",
    "        try:\n",
    "            abstract_div = soup.find('div', class_='span4', text=lambda t: t and \"Employee\" in t)\n",
    "            incident_title = extract_text(abstract_div)\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "        summary = None\n",
    "        try:\n",
    "            summary_tag = soup.find('p', string=lambda s: s and (s.startswith(\"At\") or s.startswith(\"On\")))\n",
    "            summary = extract_text(summary_tag)\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "        naics = None\n",
    "        try:\n",
    "            naics_tag = soup.find('strong', string=\"NAICS\").next_sibling.strip()\n",
    "            if naics_tag:\n",
    "                naics = naics_tag.lstrip(\":\").strip()\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "        event_date = None\n",
    "        try:\n",
    "            event_date_div = soup.find('div', class_='span4')\n",
    "            for div in soup.find_all('div', class_='span4'):\n",
    "                strong_tag = div.find('strong')\n",
    "                if strong_tag and \"Event\" in strong_tag.text:\n",
    "                    event_date = div.text.replace(\"Event\", \"\").replace(\":\", \"\").strip()\n",
    "                    break\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "        age = extract_text(soup.find('td', headers='a1 col2'))\n",
    "        sex = extract_text(soup.find('td', headers='a1 col3'))\n",
    "        degree_of_injury = extract_text(soup.find('td', headers='a1 col4'))\n",
    "        occupation = extract_text(soup.find('td', headers='a1 col6'))\n",
    "\n",
    "        OSHA_data.append({\n",
    "            'Inspection Number': inspection_id,\n",
    "            'Case Status': case_status,\n",
    "            'Establishment Name': establishment_name,\n",
    "            'Report ID': report_id,\n",
    "            'Site Address': site_address,\n",
    "            'Union Status': union_status,\n",
    "            'Incident Title': incident_title,\n",
    "            'Summary': summary,\n",
    "            'NAICS': naics,\n",
    "            'Event Date': event_date,\n",
    "            'Age': age,\n",
    "            'Sex': sex,\n",
    "            'Degree of Injury': degree_of_injury,\n",
    "            'Occupation': occupation,\n",
    "        })\n",
    "\n",
    "        print(f\"Details collected for Inspection ID: {inspection_id} ({index}/{total})\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing data for {inspection_id}: {e}\")\n",
    "        OSHA_data.append({'Inspection Number': inspection_id})\n",
    "\n",
    "    time.sleep(5)\n",
    "\n",
    "df = pd.DataFrame(OSHA_data)\n",
    "df.to_csv(\"inspection_details.csv\", index=False)\n",
    "print(\"Data saved to inspection_details.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97458428-86b0-450c-a600-b0846a39024f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping details for Inspection ID: 1756686.015...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c9/5dg8dy915gl599_ymwjvm4p80000gn/T/ipykernel_36540/1920918932.py:51: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  abstract_div = soup.find('div', class_='span4', text=lambda t: t and \"Employee\" in t)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OSHA_data collected for Inspection ID: 1756686.015\n",
      "Scraping details for Inspection ID: 1754345.015...\n",
      "OSHA_data collected for Inspection ID: 1754345.015\n",
      "Scraping details for Inspection ID: 1755843.015...\n",
      "OSHA_data collected for Inspection ID: 1755843.015\n",
      "Scraping details for Inspection ID: 1753424.015...\n",
      "OSHA_data collected for Inspection ID: 1753424.015\n",
      "Scraping details for Inspection ID: 1750189.015...\n",
      "OSHA_data collected for Inspection ID: 1750189.015\n",
      "Scraping details for Inspection ID: 1750111.015...\n",
      "OSHA_data collected for Inspection ID: 1750111.015\n",
      "Scraping details for Inspection ID: 1742027.015...\n",
      "OSHA_data collected for Inspection ID: 1742027.015\n",
      "Scraping details for Inspection ID: 1715319.015...\n",
      "OSHA_data collected for Inspection ID: 1715319.015\n",
      "Scraping details for Inspection ID: 1698609.015...\n",
      "OSHA_data collected for Inspection ID: 1698609.015\n",
      "Scraping details for Inspection ID: 1698692.015...\n",
      "OSHA_data collected for Inspection ID: 1698692.015\n",
      "Scraping details for Inspection ID: 1694606.015...\n",
      "OSHA_data collected for Inspection ID: 1694606.015\n",
      "Scraping details for Inspection ID: 1693972.015...\n",
      "OSHA_data collected for Inspection ID: 1693972.015\n",
      "Scraping details for Inspection ID: 1694715.015...\n",
      "OSHA_data collected for Inspection ID: 1694715.015\n",
      "Scraping details for Inspection ID: 1692722.015...\n",
      "OSHA_data collected for Inspection ID: 1692722.015\n",
      "Scraping details for Inspection ID: 1695215.015...\n",
      "OSHA_data collected for Inspection ID: 1695215.015\n",
      "Scraping details for Inspection ID: 1694385.015...\n",
      "OSHA_data collected for Inspection ID: 1694385.015\n",
      "Scraping details for Inspection ID: 1692708.015...\n",
      "OSHA_data collected for Inspection ID: 1692708.015\n",
      "Scraping details for Inspection ID: 1693210.015...\n",
      "OSHA_data collected for Inspection ID: 1693210.015\n",
      "Scraping details for Inspection ID: 1690736.015...\n",
      "OSHA_data collected for Inspection ID: 1690736.015\n",
      "Scraping details for Inspection ID: 1694845.015...\n",
      "OSHA_data collected for Inspection ID: 1694845.015\n",
      "OSHA_data saved to inspection_details.csv\n"
     ]
    }
   ],
   "source": [
    "for inspection_id in inspection_numbers:\n",
    "    print(f\"Scraping details for Inspection ID: {inspection_id}...\")\n",
    "\n",
    "    response = fetch_with_retries(base_url + inspection_id, headers)\n",
    "    if not response:\n",
    "        continue\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    try:\n",
    "        case_status = extract_text(soup.find('div', class_='well well-small').find('strong'))\n",
    "        if case_status:\n",
    "            case_status = case_status.replace(\"Case Status:\", \"\").strip()\n",
    "\n",
    "        inspection_number = inspection_id\n",
    "\n",
    "        establishment_name_tag = soup.find('h4', id=inspection_id)\n",
    "        establishment_name = extract_text(establishment_name_tag).split(\"-\", 1)[-1].strip() if establishment_name_tag else None\n",
    "\n",
    "        report_id = None\n",
    "        try:\n",
    "            report_id = soup.find('strong', string=\"Report ID\").next_sibling.strip().replace(\": \", \"\")\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "        site_address = None\n",
    "        try:\n",
    "            span4_divs = soup.find_all('div', class_='span4')\n",
    "        \n",
    "            for div in span4_divs:\n",
    "                paragraphs = div.find_all('p')\n",
    "                for paragraph in paragraphs:\n",
    "                    strong_tag = paragraph.find('strong')\n",
    "                    if strong_tag and \"Site Address\" in strong_tag.text:\n",
    "                        address_lines = [line.strip() for line in paragraph.stripped_strings]\n",
    "                        site_address = \"\\n\".join(line.lstrip(\":\").strip() for line in address_lines[3:])\n",
    "                        break\n",
    "                if site_address:\n",
    "                    break\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting Site Address: {e}\")\n",
    "\n",
    "        union_status = None\n",
    "        try:\n",
    "            union_status = soup.find('strong', string=\"Union Status\").next_sibling.strip().replace(\": \", \"\")\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "        incident_title = None\n",
    "        try:\n",
    "            abstract_div = soup.find('div', class_='span4', text=lambda t: t and \"Employee\" in t)\n",
    "            incident_title = extract_text(abstract_div)\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "        summary = None\n",
    "        try:\n",
    "            summary_tag = soup.find('p', string=lambda s: s and (s.startswith(\"At\") or s.startswith(\"On\")))\n",
    "            summary = extract_text(summary_tag)\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "        naics = None\n",
    "        try:\n",
    "            naics_tag = soup.find('strong', string=\"NAICS\").next_sibling.strip()\n",
    "            if naics_tag:\n",
    "                naics = naics_tag.lstrip(\":\").strip()\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "        age = extract_text(soup.find('td', headers='a1 col2'))\n",
    "        sex = extract_text(soup.find('td', headers='a1 col3'))\n",
    "        degree_of_injury = extract_text(soup.find('td', headers='a1 col4'))\n",
    "        occupation = extract_text(soup.find('td', headers='a1 col6'))\n",
    "\n",
    "        OSHA_data.append({\n",
    "            'Case Status': case_status,\n",
    "            'Inspection Number': inspection_number,\n",
    "            'Establishment Name': establishment_name,\n",
    "            'Report ID': report_id,\n",
    "            'Site Address': site_address,\n",
    "            'Union Status': union_status,\n",
    "            'Incident Title': incident_title,\n",
    "            'Summary': summary,\n",
    "            'NAICS': naics,\n",
    "            'Age': age,\n",
    "            'Sex': sex,\n",
    "            'Degree of Injury': degree_of_injury,\n",
    "            'Occupation': occupation,\n",
    "        })\n",
    "\n",
    "        print(f\"OSHA_data collected for Inspection ID: {inspection_id}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing OSHA_data for {inspection_id}: {e}\")\n",
    "\n",
    "    time.sleep(5)\n",
    "\n",
    "df = pd.DataFrame(OSHA_data)\n",
    "df.to_csv(\"inspection_details.csv\", index=False)\n",
    "print(\"OSHA_data saved to inspection_details.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fb2ea6-7721-4e04-89dc-3e364c9bb4c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55ff0e4-31ef-4d34-86e8-2a8f80b9739f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd60875b-ac7b-4914-8b2a-b6cf219afd62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32d053b-f18b-41de-ae6b-fbd40e4f1c1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
